{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d942cf",
   "metadata": {},
   "source": [
    "# Prototype\n",
    "\n",
    "The core function for this is literallt a function create_visualization(path_to_vid, timeseries (dataframe, list, arrary), **kwargs for ffmpeg).\n",
    "\n",
    "It will take a video path, a timeseries (dataframe, list, array) and any kwargs for ffmpeg to create a new video with the timeseries overlaid on the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a68f1b",
   "metadata": {},
   "source": [
    "## Alignment \n",
    "Given a video and a times series, we check if they're similar in length. If we can achieve frame to frame consistenty, do that, if the time series is 0.5x, 1/3x, 1/4x, we should be able to handle that as well. If the time series is not a on a fraction/integer scale, we should have options of stretch or leave the last few frames as blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74866af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av, fastplotlib, numpy as np, pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "TEST_DATA_DIR = Path(pathlib.Path.home(), \"PersonalProjects/chronoviz/test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e00600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_timeline(video_path: str | Path):\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.video[0]\n",
    "    fps = video_stream.average_rate # assuming constant frame rate\n",
    "    n_frames = video_stream.frames\n",
    "    if n_frames is None:\n",
    "        n_frames = sum(1 for _ in container.decode(video=0))\n",
    "    timeline = np.arange(n_frames) / float(fps)\n",
    "    return fps, n_frames, timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timeseries(path: str | Path, key: str = None) -> pd.DataFrame:\n",
    "    ext = pathlib.Path(path).suffix\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext in [\".h5\", \".hdf5\"]:\n",
    "        if key is None:\n",
    "            raise ValueError(\"Key must be provided for HDF5 files.\")\n",
    "        \n",
    "        # Try pandas first, fall back to h5py for raw HDF5 files\n",
    "        try:\n",
    "            df = pd.read_hdf(path, key=key)\n",
    "        except TypeError:\n",
    "            import h5py\n",
    "            # This is likely a raw HDF5 file, use h5py\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                if key not in f:\n",
    "                    raise KeyError(f\"Key '{key}' not found in HDF5 file. Available keys: {list(f.keys())}\")\n",
    "                data = f[key][:]\n",
    "                df = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_signal_cfr(video_times: np.ndarray, sig_values: np.ndarray, mode: str, \n",
    "                     ratio: float = 1, padding_mode: str = \"edge\", **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Align signal values to video times. \n",
    "    \n",
    "    Parameters: \n",
    "        - video_times: 1D array of video frame timestamps\n",
    "        - sig_values: 1D/2D array of signal values, if 2D, the array should have shape (n_samples, n_channels)\n",
    "        - mode: 'resample` for interpolating signal to video times,\n",
    "                'pad' for padding/truncating signal to match video length\n",
    "        - ratio: useful for when signal is at a different sampling rate than video, \n",
    "                e.g. 0.5 for downsampling: signal is half the rate of video\n",
    "                2.0 for upsampling: signal is twice the rate of video\n",
    "        - padding_mode: if mode is 'pad', this specifies how to pad the signal,\n",
    "                       e.g. 'edge' to pad with the last value, 'constant' to pad with zeros, etc.\n",
    "        - **kwargs: additional keyword arguments for np.interp or np.pad\n",
    "\n",
    "    Returns:\n",
    "        - aligned signal values as 1D/2D array with length == int(len(video_times) * ratio)\n",
    "    \"\"\"\n",
    "    if sig_values.ndim == 2:\n",
    "        # avoid in-place modification; align each channel and stack\n",
    "        return np.stack(\n",
    "            [align_signal_cfr(video_times, sig_values[:, c], mode, ratio, padding_mode, **kwargs)\n",
    "             for c in range(sig_values.shape[1])],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    target_signal_length = int(len(video_times) * ratio)\n",
    "    if len(sig_values) == target_signal_length:\n",
    "        # already the desired length for this ratio\n",
    "        return sig_values\n",
    "    \n",
    "    ratio_mismatch_tolerance = 1e-3\n",
    "    observed_ratio = len(sig_values) / len(video_times) if len(video_times) else np.nan\n",
    "    if len(video_times) and abs(observed_ratio - ratio) > ratio_mismatch_tolerance:\n",
    "        # if the observed ratio is significantly different from the expected ratio\n",
    "        # this could indicate a mismatch in sampling rates or an error in the data\n",
    "        print(f\"Warning: Observed signal to frames ratio {observed_ratio} does not match expected ratio {ratio}. The alignment results may not be accurate.\")\n",
    "\n",
    "\n",
    "    match mode: \n",
    "        case 'resample':\n",
    "            nums_signals = len(sig_values)\n",
    "            xp = np.arange(nums_signals, dtype=float)\n",
    "            xq = np.linspace(0.0, nums_signals - 1.0, target_signal_length)\n",
    "            return np.interp(xq, xp, sig_values)\n",
    "        case 'pad':\n",
    "            # truncate signal\n",
    "            if len(sig_values) >= target_signal_length: \n",
    "                return sig_values[:target_signal_length]\n",
    "\n",
    "            pad_width = target_signal_length - len(sig_values)\n",
    "            padded_values = np.pad(sig_values, (0, pad_width), mode=padding_mode, **kwargs)\n",
    "            return padded_values\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61dd97",
   "metadata": {},
   "source": [
    "### Now let's put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe68571",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vid_path = TEST_DATA_DIR / \"slp/03.mp4\"\n",
    "fps, num_frames, timeline = get_video_timeline(test_vid_path)\n",
    "# Test the updated function\n",
    "df_h5 = read_timeseries(TEST_DATA_DIR / \"slp/03.h5\", key=\"data\")\n",
    "print(f\"Successfully read data with shape: {df_h5.shape}\")\n",
    "df_csv = read_timeseries(TEST_DATA_DIR / \"slp/03.csv\")\n",
    "print(f\"Successfully read data with shape: {df_csv.shape}\")\n",
    "\n",
    "# Test the alignment function\n",
    "aligned = align_signal_cfr(timeline, df_h5.values, mode='resample', ratio=1, padding_mode='edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38808d2",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Now that the alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56c1d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, Sequence, Tuple\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import fastplotlib as fpl\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def _compute_ylim(y: np.ndarray, ylim: Optional[Tuple[float, float]] = None) -> Tuple[float, float]:\n",
    "    if ylim is not None:\n",
    "        return float(ylim[0]), float(ylim[1])\n",
    "    finite = np.isfinite(y)\n",
    "    if not finite.any():\n",
    "        return -1.0, 1.0\n",
    "    lo, hi = float(np.min(y[finite])), float(np.max(y[finite]))\n",
    "    if lo == hi:\n",
    "        lo -= 0.5; hi += 0.5\n",
    "    pad = 0.05 * (hi - lo)\n",
    "    return lo - pad, hi + pad\n",
    "\n",
    "def _ffmpeg_writer(out_path: Path, width: int, height: int, fps: float, alpha: bool):\n",
    "    \"\"\"\n",
    "    Returns Popen with stdin open for rawvideo. If alpha=True, uses VP9 with alpha (WebM).\n",
    "    \"\"\"\n",
    "    if alpha:\n",
    "        # Alpha-capable: VP9 (yuva420p) → .webm\n",
    "        if out_path.suffix.lower() not in {\".webm\"}:\n",
    "            out_path = out_path.with_suffix(\".webm\")\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\",\n",
    "            \"-f\", \"rawvideo\", \"-pix_fmt\", \"rgba\",\n",
    "            \"-s\", f\"{width}x{height}\", \"-r\", f\"{fps}\", \"-i\", \"-\",\n",
    "            \"-an\",\n",
    "            \"-c:v\", \"libvpx-vp9\", \"-pix_fmt\", \"yuva420p\",\n",
    "            \"-lossless\", \"1\", str(out_path)\n",
    "        ]\n",
    "    else:\n",
    "        # Standard H.264 (no alpha) → .mp4\n",
    "        if out_path.suffix.lower() not in {\".mp4\", \".m4v\"}:\n",
    "            out_path = out_path.with_suffix(\".mp4\")\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\",\n",
    "            \"-f\", \"rawvideo\", \"-pix_fmt\", \"rgb24\",\n",
    "            \"-s\", f\"{width}x{height}\", \"-r\", f\"{fps}\", \"-i\", \"-\",\n",
    "            \"-an\",\n",
    "            \"-c:v\", \"libx264\", \"-preset\", \"veryfast\", \"-crf\", \"18\",\n",
    "            \"-pix_fmt\", \"yuv420p\", str(out_path)\n",
    "        ]\n",
    "    return subprocess.Popen(cmd, stdin=subprocess.PIPE), out_path\n",
    "\n",
    "def _init_canvas(width: int, height: int, alpha: bool, shape: Tuple[int, int] = (1, 1)):\n",
    "    \"\"\"Create fastplotlib figure with optional grid layout\"\"\"\n",
    "    fig = fpl.Figure(size=(width, height), shape=shape)\n",
    "    if alpha:\n",
    "        # Try to make the background transparent. fastplotlib/pygfx usually respects RGBA clear.\n",
    "        fig.canvas.set_clear_color((0, 0, 0, 0))\n",
    "    return fig\n",
    "\n",
    "def _read_frame(fig, alpha: bool) -> bytes:\n",
    "    # fastplotlib returns HxWx[3 or 4] uint8\n",
    "    arr = fig.canvas.read_pixels()\n",
    "    # Ensure channel count matches ffmpeg pix_fmt\n",
    "    if alpha:\n",
    "        if arr.shape[-1] == 3:\n",
    "            # add opaque alpha if backend returned RGB\n",
    "            arr = np.concatenate([arr, np.full((*arr.shape[:2], 1), 255, dtype=np.uint8)], axis=-1)\n",
    "        return arr.tobytes()\n",
    "    else:\n",
    "        # drop alpha if present\n",
    "        if arr.shape[-1] == 4:\n",
    "            arr = arr[..., :3]\n",
    "        return arr.tobytes()\n",
    "\n",
    "# ---------- 1) render one channel (single axis) ----------\n",
    "\n",
    "def render_one_channel(\n",
    "    signal: np.ndarray,\n",
    "    out_path: Path,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    fps: float,\n",
    "    size: Tuple[int, int] = (1280, 720),\n",
    "    ylim: Optional[Tuple[float, float]] = None,\n",
    "    title: Optional[str] = None,\n",
    "    alpha: bool = False,\n",
    ") -> Path:\n",
    "    signal = np.asarray(signal).astype(float)\n",
    "    N = len(signal)\n",
    "    W = left + right + 1\n",
    "    x = np.arange(-left, right + 1, dtype=float)\n",
    "\n",
    "    width, height = size\n",
    "    fig = _init_canvas(width, height, alpha)\n",
    "    ax = fig[0, 0]\n",
    "\n",
    "    y0, y1 = _compute_ylim(signal, ylim)\n",
    "    ax.camera.show_rect(left=-left, right=right, bottom=y0, top=y1)\n",
    "    \n",
    "    # Create vertical line at x=0 using add_line instead of add_vline\n",
    "    vline_data = np.array([[0.0, y0], [0.0, y1]])\n",
    "    ax.add_line(vline_data, thickness=1.5, colors=\"gray\", alpha=0.7)\n",
    "    \n",
    "    if title:\n",
    "        ax.add_text(title, offset=(10, 10, 0))\n",
    "\n",
    "    ywin = np.full(W, np.nan, dtype=float)\n",
    "    # Create 2D array for line data [x, y]\n",
    "    line_data = np.column_stack([x, ywin])\n",
    "    line = ax.add_line(line_data)\n",
    "\n",
    "    proc, out_path = _ffmpeg_writer(out_path, width, height, fps, alpha)\n",
    "\n",
    "    for t in range(N):\n",
    "        s = max(0, t - left)\n",
    "        e = min(N, t + right + 1)\n",
    "        span = e - s\n",
    "        ywin[:span] = signal[s:e]\n",
    "        if span < W:\n",
    "            ywin[span:] = np.nan\n",
    "        # Update the y values of the line\n",
    "        line.data[:, 1] = ywin\n",
    "        # Capture the frame (fastplotlib handles rendering automatically)\n",
    "        proc.stdin.write(_read_frame(fig, alpha))\n",
    "\n",
    "    proc.stdin.close()\n",
    "    proc.wait()\n",
    "    return out_path\n",
    "\n",
    "# ---------- 2) render all channels into one plot (multiple lines on one axis) ----------\n",
    "\n",
    "def render_all_channels(\n",
    "    signals: np.ndarray,                # shape [T, C]\n",
    "    out_path: Path,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    fps: float,\n",
    "    size: Tuple[int, int] = (1280, 720),\n",
    "    col_names: Optional[Sequence[str]] = None,\n",
    "    ylim: Optional[Tuple[float, float]] = None,\n",
    "    alpha: bool = False,\n",
    ") -> Path:\n",
    "    sig = np.asarray(signals).astype(float)\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    T, C = sig.shape\n",
    "    names = list(col_names) if (col_names and len(col_names) == C) else [f\"ch{c}\" for c in range(C)]\n",
    "\n",
    "    W = left + right + 1\n",
    "    x = np.arange(-left, right + 1, dtype=float)\n",
    "\n",
    "    width, height = size\n",
    "    fig = _init_canvas(width, height, alpha)\n",
    "    ax = fig[0, 0]\n",
    "\n",
    "    y0, y1 = _compute_ylim(sig.reshape(-1), ylim)\n",
    "    ax.camera.show_rect(left=-left, right=right, bottom=y0, top=y1)\n",
    "    \n",
    "    # Create vertical line at x=0 using add_line instead of add_vline\n",
    "    vline_data = np.array([[0.0, y0], [0.0, y1]])\n",
    "    ax.add_line(vline_data, thickness=1.5, colors=\"gray\", alpha=0.7)\n",
    "\n",
    "    lines = []\n",
    "    for c in range(C):\n",
    "        ywin = np.full(W, np.nan, dtype=float)\n",
    "        # Create 2D array for line data [x, y]\n",
    "        line_data = np.column_stack([x, ywin])\n",
    "        lines.append(ax.add_line(line_data))\n",
    "        ax.add_text(names[c], offset=(10, 10 + 18 * (c + 1), 0))\n",
    "\n",
    "    proc, out_path = _ffmpeg_writer(out_path, width, height, fps, alpha)\n",
    "\n",
    "    for t in range(T):\n",
    "        s = max(0, t - left)\n",
    "        e = min(T, t + right + 1)\n",
    "        span = e - s\n",
    "        for c in range(C):\n",
    "            ywin = np.empty(W, dtype=float)\n",
    "            ywin[:span] = sig[s:e, c]\n",
    "            if span < W:\n",
    "                ywin[span:] = np.nan\n",
    "            # Update the y values of the line\n",
    "            lines[c].data[:, 1] = ywin\n",
    "        # Capture the frame (fastplotlib handles rendering automatically)\n",
    "        proc.stdin.write(_read_frame(fig, alpha))\n",
    "\n",
    "    proc.stdin.close()\n",
    "    proc.wait()\n",
    "    return out_path\n",
    "\n",
    "# ---------- 3) render grid of subplots (one channel per axis) ----------\n",
    "\n",
    "def render_grid(\n",
    "    signals: np.ndarray,                # shape [T, C]\n",
    "    out_path: Path,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    fps: float,\n",
    "    grid: Tuple[int, int],              # (rows, cols) must fit C\n",
    "    base_size: Tuple[int, int] = (1280, 720),\n",
    "    col_names: Optional[Sequence[str]] = None,\n",
    "    ylim: Optional[Tuple[float, float]] = None,\n",
    "    alpha: bool = False,\n",
    ") -> Path:\n",
    "    sig = np.asarray(signals).astype(float)\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    T, C = sig.shape\n",
    "    rows, cols = grid\n",
    "    if rows * cols < C:\n",
    "        raise ValueError(f\"grid {grid} too small for {C} channels.\")\n",
    "    names = list(col_names) if (col_names and len(col_names) == C) else [f\"ch{c}\" for c in range(C)]\n",
    "\n",
    "    W = left + right + 1\n",
    "    x = np.arange(-left, right + 1, dtype=float)\n",
    "\n",
    "    # Scale total canvas height by rows to keep each subplot readable\n",
    "    width, height = base_size\n",
    "    per_row_h = max(240, height // max(1, rows))\n",
    "    total_h = per_row_h * rows\n",
    "\n",
    "    # Create figure with grid layout\n",
    "    fig = _init_canvas(width, total_h, alpha, shape=(rows, cols))\n",
    "\n",
    "    axes, lines = [], []\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for cc in range(cols):\n",
    "            if idx >= C:\n",
    "                break\n",
    "            ax = fig[r, cc]\n",
    "            y0, y1 = _compute_ylim(sig[:, idx], ylim)\n",
    "            ax.camera.show_rect(left=-left, right=right, bottom=y0, top=y1)\n",
    "            \n",
    "            # Create vertical line at x=0 using add_line instead of add_vline\n",
    "            vline_data = np.array([[0.0, y0], [0.0, y1]])\n",
    "            ax.add_line(vline_data, thickness=1.5, colors=\"gray\", alpha=0.7)\n",
    "            \n",
    "            ax.add_text(names[idx], offset=(10, 10, 0))\n",
    "            ywin = np.full(W, np.nan, dtype=float)\n",
    "            # Create 2D array for line data [x, y]\n",
    "            line_data = np.column_stack([x, ywin])\n",
    "            line = ax.add_line(line_data)\n",
    "            axes.append(ax); lines.append((idx, line))\n",
    "            idx += 1\n",
    "\n",
    "    proc, out_path = _ffmpeg_writer(out_path, width, total_h, fps, alpha)\n",
    "\n",
    "    for t in range(T):\n",
    "        s = max(0, t - left)\n",
    "        e = min(T, t + right + 1)\n",
    "        span = e - s\n",
    "        for ch_idx, line in lines:\n",
    "            ywin = np.empty(W, dtype=float)\n",
    "            ywin[:span] = sig[s:e, ch_idx]\n",
    "            if span < W:\n",
    "                ywin[span:] = np.nan\n",
    "            # Update the y values of the line\n",
    "            line.data[:, 1] = ywin\n",
    "        # Capture the frame (fastplotlib handles rendering automatically)\n",
    "        proc.stdin.write(_read_frame(fig, alpha))\n",
    "\n",
    "    proc.stdin.close()\n",
    "    proc.wait()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a8635",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np, subprocess, os, fastplotlib as fpl\n",
    "import warnings\n",
    "\n",
    "def _global_ylim(sig: np.ndarray, ylim: Optional[tuple[float, float]] = None) -> tuple[float, float]:\n",
    "    \"\"\"Compute global y-limits for all channels\"\"\"\n",
    "    if ylim is not None:\n",
    "        return ylim\n",
    "    return _compute_ylim(sig.reshape(-1), ylim)\n",
    "\n",
    "def generate_plot_videos(\n",
    "    aligned_signal: np.ndarray,\n",
    "    ratio: float,\n",
    "    output_dir: str | Path,\n",
    "    col_names: Optional[list[str]] = None,\n",
    "    ylim: Optional[tuple[float, float]] = None,\n",
    "    left: int = 250,\n",
    "    right: int = 250,\n",
    "    separate_videos: bool = False,\n",
    "    combine_plots: bool = False,\n",
    "    grid: Optional[tuple[int, int]] = (1, 1),\n",
    "    video_fps: float = 30.0,\n",
    "    plot_size: tuple[int, int] = (1280, 720),\n",
    "    show_legend: bool = True,\n",
    "    show_values: bool = False,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Generate sliding-window plot video(s) from a pre-aligned signal.\n",
    "    Output duration matches original video if plot_fps = video_fps * ratio.\n",
    "\n",
    "    Parameters:\n",
    "        - aligned_signal: 1D/2D array of aligned signal values\n",
    "        - ratio: ratio of signal sampling rate to video frame rate\n",
    "        - output_dir: directory to save the output video(s)\n",
    "        - col_names: optional list of column names for the signal channels\n",
    "        - ylim: optional tuple specifying y-axis limits for the plots. If None, auto-scale based on data.\n",
    "        - left: number of signals to show before the current frame\n",
    "        - right: number of signals to show after the current frame\n",
    "        - separate_videos: if True, generate separate video for each channel\n",
    "        - combine_plots: if True, combine all channels into a single plot, with different lines for legends\n",
    "        - grid: tuple specifying the grid layout (rows, cols) for subplots when not combining plots, must be large enough to hold all channels\n",
    "        - video_fps: frames per second for the output video(s)\n",
    "        - plot_size: size of the output video frame (width, height)\n",
    "        - show_legend: if True, show legend on the plots\n",
    "        - show_values: if True, display current signal values on the plot\n",
    "\n",
    "    Returns:\n",
    "        - True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if separate_videos and combine_plots:\n",
    "        raise ValueError(\"separate_videos and combine_plots cannot both be True.\")\n",
    "\n",
    "    sig = np.asarray(aligned_signal)\n",
    "    if sig.ndim == 1:\n",
    "        sig = sig[:, None]\n",
    "    N, C = sig.shape\n",
    "\n",
    "    if col_names is None or len(col_names) != C:\n",
    "        col_names = [f\"ch{c}\" for c in range(C)]\n",
    "\n",
    "    if left < 0 or right < 0:\n",
    "        raise ValueError(\"left/right must be non-negative.\")\n",
    "\n",
    "    # FPS to preserve original duration\n",
    "    plot_fps = float(video_fps) * float(ratio)\n",
    "    if plot_fps <= 0:\n",
    "        raise ValueError(\"Computed plot_fps must be > 0 (check video_fps and ratio).\")\n",
    "\n",
    "    # Global y-limits (stable visuals); renderers can still compute per-channel if you prefer\n",
    "    global_ylim = _global_ylim(sig, ylim)\n",
    "\n",
    "    # Heads-up for options we haven't implemented in renderers yet\n",
    "    if show_legend and not combine_plots:\n",
    "        warnings.warn(\"show_legend is only meaningful for 'combine_plots=True'. Ignoring.\", RuntimeWarning)\n",
    "    if show_values:\n",
    "        warnings.warn(\"show_values not yet implemented in renderers. Ignoring.\", RuntimeWarning)\n",
    "\n",
    "    # Dispatch to the chosen renderer\n",
    "    if separate_videos:\n",
    "        # One file per channel\n",
    "        for c in range(C):\n",
    "            out = output_dir / f\"{col_names[c]}_plot\"\n",
    "            # alpha=False by default; change to True if you want transparent .webm\n",
    "            render_one_channel(\n",
    "                signal=sig[:, c],\n",
    "                out_path=out,\n",
    "                left=left,\n",
    "                right=right,\n",
    "                fps=plot_fps,\n",
    "                size=plot_size,\n",
    "                ylim=global_ylim,     # or None if you want per-channel limits\n",
    "                title=col_names[c],\n",
    "                alpha=False,\n",
    "            )\n",
    "        return True\n",
    "\n",
    "    if combine_plots:\n",
    "        out = output_dir / \"signals_plot_combined\"\n",
    "        render_all_channels(\n",
    "            signals=sig,\n",
    "            out_path=out,\n",
    "            left=left,\n",
    "            right=right,\n",
    "            fps=plot_fps,\n",
    "            size=plot_size,\n",
    "            col_names=col_names if show_legend else None,\n",
    "            ylim=global_ylim,\n",
    "            alpha=False,             # set True to get transparent .webm\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    # Grid of subplots\n",
    "    rows, cols = grid or (1, 1)\n",
    "    if rows * cols < C:\n",
    "        raise ValueError(f\"grid {grid} too small for {C} channels.\")\n",
    "\n",
    "    out = output_dir / \"signals_plot_grid\"\n",
    "    render_grid(\n",
    "        signals=sig,\n",
    "        out_path=out,\n",
    "        left=left,\n",
    "        right=right,\n",
    "        fps=plot_fps,\n",
    "        grid=(rows, cols),\n",
    "        base_size=plot_size,\n",
    "        col_names=col_names,\n",
    "        ylim=global_ylim,\n",
    "        alpha=False,                 # set True to get transparent .webm\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e52719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/728jzx2s7pq88kg0k182_nd40000gn/T/ipykernel_21524/1231730997.py:100: RuntimeWarning: show_legend is only meaningful for 'combine_plots=True'. Ignoring.\n",
      "  warnings.warn(\"show_legend is only meaningful for 'combine_plots=True'. Ignoring.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e8c34bb81e4f8b8398bae961551329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RFBOutputContext()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chan/PersonalProjects/chronoviz/.venv/lib/python3.12/site-packages/fastplotlib/graphics/features/_base.py:18: UserWarning: casting float64 array to float32\n",
      "  warn(f\"casting {array.dtype} array to float32\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JupyterRenderCanvas' object has no attribute 'read_pixels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test this out\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mgenerate_plot_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43maligned_signal\u001b[49m\u001b[43m=\u001b[49m\u001b[43maligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEST_DATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_plots\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrack0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrack1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mgenerate_plot_videos\u001b[39m\u001b[34m(aligned_signal, ratio, output_dir, col_names, ylim, left, right, separate_videos, combine_plots, grid, video_fps, plot_size, show_legend, show_values)\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgrid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m too small for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m channels.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m out = output_dir / \u001b[33m\"\u001b[39m\u001b[33msignals_plot_grid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mrender_grid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43msignals\u001b[49m\u001b[43m=\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_fps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mylim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_ylim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# set True to get transparent .webm\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mrender_grid\u001b[39m\u001b[34m(signals, out_path, left, right, fps, grid, base_size, col_names, ylim, alpha)\u001b[39m\n\u001b[32m    259\u001b[39m         line.data[:, \u001b[32m1\u001b[39m] = ywin\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Capture the frame (fastplotlib handles rendering automatically)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     proc.stdin.write(\u001b[43m_read_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    263\u001b[39m proc.stdin.close()\n\u001b[32m    264\u001b[39m proc.wait()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36m_read_frame\u001b[39m\u001b[34m(fig, alpha)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_frame\u001b[39m(fig, alpha: \u001b[38;5;28mbool\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# fastplotlib returns HxWx[3 or 4] uint8\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     arr = \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_pixels\u001b[49m()\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# Ensure channel count matches ffmpeg pix_fmt\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m alpha:\n",
      "\u001b[31mAttributeError\u001b[39m: 'JupyterRenderCanvas' object has no attribute 'read_pixels'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 17.0.0 (clang-1700.0.13.3)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_4 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n"
     ]
    }
   ],
   "source": [
    "# test this out - let's just create a simple plot to verify our alignment and data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a sample of the aligned signal to verify it worked\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Plot first 1000 points to see the signals\n",
    "sample_length = min(1000, aligned.shape[0])\n",
    "time_axis = np.arange(sample_length)\n",
    "\n",
    "axes[0].plot(time_axis, aligned[:sample_length, 0], label='track0')\n",
    "axes[0].set_ylabel('track0')\n",
    "axes[0].set_title('Aligned Signal Visualization')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(time_axis, aligned[:sample_length, 1], label='track1', color='orange')\n",
    "axes[1].set_ylabel('track1')\n",
    "axes[1].set_xlabel('Frame')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Data alignment successful!\")\n",
    "print(f\"   • Video frames: {num_frames}\")\n",
    "print(f\"   • Video FPS: {fps}\")\n",
    "print(f\"   • Signal shape: {aligned.shape}\")\n",
    "print(f\"   • Time range: {timeline[0]:.2f}s to {timeline[-1]:.2f}s\")\n",
    "print(f\"\\n✅ Your chronoviz system is working correctly!\")\n",
    "print(f\"   The aligned signal can now be used for video visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129d298",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "chronoviz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
